{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgfACKj4gX3w5ZNQU72qgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noumantechie/langchain/blob/main/memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Configurable Paramter(Means those parrameter that change on runtime) **"
      ],
      "metadata": {
        "id": "vG92W80uy-mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "id": "Ruj0BX7fgXFH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we add histroy as a context to LLM by MessagePlaceholder**"
      ],
      "metadata": {
        "id": "OF8zP0Q-lFRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain_core.runnables import ConfigurableField"
      ],
      "metadata": {
        "id": "5cMWwmykg1uM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "SwnSQpB42W78"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Default model\n",
        "    google_api_key=\"Your API KEY\"\n",
        ").configurable_fields(\n",
        "    model=ConfigurableField(\n",
        "        id=\"model_name\",\n",
        "        name=\"Model Name\",\n",
        "        description=\"The Gemini model to use\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "jgeC71l73SFE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chain\n",
        "prompt = PromptTemplate.from_template(\"Write a Haiku on {subject}\")\n",
        "chain = prompt | llm\n"
      ],
      "metadata": {
        "id": "P_iL0qPvlkRi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First invocation - default model\n",
        "print(\"\\n=== First Invocation (Default Model) ===\")\n",
        "response1 = chain.invoke({\"subject\": \"cat\"})\n",
        "print(response1.content)\n",
        "print(f\"Model User{llm.model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUNHm1_3jHyY",
        "outputId": "90f7036c-188a-4605-cc50-2cacd9a8cfba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== First Invocation (Default Model) ===\n",
            "Soft paws tread lightly,\n",
            "Emerald eyes watch silently,\n",
            "A purring rumble. \n",
            "Model Usermodels/gemini-1.5-flash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Second Invocation (Changed Model) ===\")\n",
        "configured_chain = chain.with_config(\n",
        "    configurable={\"model_name\": \"gemini-1.0-pro\"}\n",
        ")\n",
        "response2 = configured_chain.invoke({\"subject\": \"cat\"})\n",
        "print(response2.content)\n",
        "print(f\"Model Used: {configured_chain.config['configurable']['model_name']}\")  # Shows new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNwXB1ptmAD-",
        "outputId": "3225aee2-5245-4012-93aa-3ece1b3fcfd5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Second Invocation (Changed Model) ===\n",
            "Soft fur, gentle purrs\n",
            "A feline's love, warm and true\n",
            "A furry companion\n",
            "Model Used: gemini-1.0-pro\n"
          ]
        }
      ]
    }
  ]
}